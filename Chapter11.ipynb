{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2. 수렴하지 않는 활성화 함수 : Gradient 폭주/소실 방지 \n",
    "- kernel_initializer = he_normal\n",
    "- kernel_initializer = lecun_normal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성화 함수를 잘못 선택하면 자칫 그레이디언트의 소실이나 폭주\n",
    "- 은닉층 활성화 함수 성능 \n",
    "    - SELU > ELU > LeakyReLU(그리고 변종들) > ReLU > tanh> 로지스틱 순\n",
    "    \n",
    "- 과대적합 -> RReLU ( Randomized ReLU ) \n",
    "- 큰 훈련세트 -> PReLU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LeakyReLu(z) = $ max(\\alpha, z) $ \n",
    "- RReLU : $\\alpha$가 훈련하는 동안 학습\n",
    "- PReLu : $\\alpha$ 무작위 선택, 테스트시 $\\alpha$ 평균 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "- SELU : 네트워크 자기 정규화, 출력 ~ N(0, 1) \n",
    "- ELU: ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트, 사용 세트 분리 \n",
    "X_train_full, X_test, y_train_full, y_test =\\\n",
    "    train_test_split(housing.data, housing.target)\n",
    "\n",
    "# 학습, 검증 셋 분리 \n",
    "X_train, X_valid, y_train, y_valid =\\\n",
    "    train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5160, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5160,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### model 생성 \n",
    "- 글로럿/세이비어/르쿤 초기화 \n",
    "    - $ fan_{avg} = {{fan_{in} +fan_{out}}) \\over 2 }$\n",
    "- PReLU \n",
    "- selu \n",
    "- SELU > ELU > LeakyReLU(그리고 변종들) > ReLU > tanh> 로지스틱 순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    \n",
    "    # LeakyRelu \n",
    "    # 층 생성 후, 적용할 층 뒤에 LeakyReLU 적용  \n",
    "    keras.layers.Dense(10, kernel_initializer = 'he_normal'),\n",
    "    \n",
    "    keras.layers.LeakyReLU(alpha = 0.3),\n",
    "    \n",
    "    # SELU \n",
    "    keras.layers.Dense(10, kernel_initializer = 'he_normal'),\n",
    "    \n",
    "    # PReLU \n",
    "    keras.layers.PReLU(alpha_initializer='zeros'),\n",
    "    \n",
    "    # SELU \n",
    "    keras.layers.Dense(10, activation = 'selu', \n",
    "                      kernel_initializer = 'lecun_normal')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3. 배치 정규화 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련초기, 그레디언트 소실, 폭주 감소시킬 수 있음\n",
    "    - kernel_initializer = he_normal \n",
    "    - kernel_initializer = lecun_normal \n",
    "    - 하지만 훈련하는 동안, 다시 폭주/소실 발생 가능 \n",
    "- 훈련 중, 그레디언트 소실, 폭주 방지 \n",
    "    - 배치 정규화(BN) \n",
    "        - 각 층에서 활성화 함수를 통과하기 전, 후 모델에 연산 추가 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- γ(출력 스케일 벡터)와 β(출력 이동 벡터)는 일반적인 역전파를 통해 학습\n",
    "- μ(최종 입력 평균 벡터)와 σ(최종 입력 표준편 차 벡터)는 지수 이동 평균을 사용\n",
    "    - 각 층마다 4개의 파라미터 γ, β, μ, σ 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training 할 때는 mini-batch의 평균과 분산으로 normalize 하고,  \n",
    "Test 할 때는 계산해놓은 이동 평균으로 normalize 한다.\n",
    "\n",
    "- 역전파 학습 :   \n",
    "    γ(출력 스케일 벡터) / β(출력 이동 벡터) ➤ 역전파 통해 추정 \n",
    "\n",
    "- 지수 이동 평균을 통해 추정  \n",
    "    μ(최종 입력 평균 벡터) / σ(최종 입력 표준편 차 벡터) ➤ 지수 이동 평균 사용 추정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중요 파라미터 \n",
    "    - axis : 학습시키는 데이터의 컬럼 / 피쳐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 적재 \n",
    "# fashion dataset : 60000 x 28 x 28 크기 \n",
    "fashion_mnist = keras.datasets.fashion_mnist \n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 훈련세트와 테스트 세트 분리 \n",
    "X_valid = X_train_full[:5000] / 255.0\n",
    "\n",
    "X_train = X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid = y_train_full[:5000] / 255.0\n",
    "\n",
    "y_train = y_train_full[5000:] / 255.0\n",
    "\n",
    "X_test = X_test / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 활성화 함수 이전, 배치 정규과화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치정규화 추가 \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),    \n",
    "    \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer = 'he_normal'),\n",
    "    \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer = 'he_normal'),\n",
    "\n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 활성화 함수 이후, 배치 정규화 : keras.layers.BatchNormalization() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    \n",
    "#     keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 은닉층 : 활성화 함수 지정 안함, use_bias = False\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False), \n",
    "    # 배치 정규화 층 입력\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # 활성함수 입력 \n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    \n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 267,810\n",
      "Trainable params: 267,010\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\n- 아래 그림 \\n    - 정규화 배치 층 파라미터 : 784 * 4 ( γ, β, μ, σ ) = 3136 \\n    - gamma : γ / beta : β, 배치 정규화 층에서 mo\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# 정규화 층 : 784 * 4개의 파라미터가 추가됨 \n",
    "\n",
    "'''    \n",
    "- 아래 그림 \n",
    "    - 정규화 배치 층 파라미터 : 784 * 4 ( γ, β, μ, σ ) = 3136 \n",
    "    - gamma : γ / beta : β, 배치 정규화 층에서 mo\n",
    "'''\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그레이디언트 클리핑 : 배치 정규화가 어려울 때, 그레디언트 < threshold 되도록 clipping 하는 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipping \n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) \n",
    "\n",
    "# Clip norm \n",
    "# l2 norm이 지정한 임곗값보다 클 경우, 그레디언트 클리핑 \n",
    "# optimizer = keras.optimizers.SGD(clipnorm=1.0)\n",
    "\n",
    "model.compile(loss ='sparse_categorical_crossentropy',    \n",
    "              optimizer=optimizer,\n",
    "              metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 5s 98us/sample - loss: 0.0787 - accuracy: 0.0998\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.0034 - accuracy: 0.1008\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 5s 91us/sample - loss: 0.0018 - accuracy: 0.1008\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 5s 83us/sample - loss: 0.0012 - accuracy: 0.1008\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 5s 82us/sample - loss: 9.1240e-04 - accuracy: 0.1008\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 4s 76us/sample - loss: 7.2548e-04 - accuracy: 0.1008\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 4s 77us/sample - loss: 5.9903e-04 - accuracy: 0.1008\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 4s 76us/sample - loss: 5.0931e-04 - accuracy: 0.1008\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 5s 83us/sample - loss: 4.4251e-04 - accuracy: 0.1008\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 5s 85us/sample - loss: 3.9020e-04 - accuracy: 0.1008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa68791bad0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "         epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 사전훈련된 층 재사용하기 : 전이학습  ( MNIST Fashion 데이터셋 재사용 ) \n",
    "- 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아본 다음(14장 참조), 그 신경망의 하위층을 재사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 사전 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building e\n",
    "model_A = keras.models.Sequential()\n",
    "\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "    \n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.0116 - accuracy: 0.1007 - val_loss: 0.0015 - val_accuracy: 0.0914\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 5s 95us/sample - loss: 0.0011 - accuracy: 0.1008 - val_loss: 7.3474e-04 - val_accuracy: 0.0914\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 6.4517e-04 - accuracy: 0.1008 - val_loss: 4.8310e-04 - val_accuracy: 0.0914\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 5s 91us/sample - loss: 4.5314e-04 - accuracy: 0.1008 - val_loss: 3.5847e-04 - val_accuracy: 0.0914\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: 3.4859e-04 - accuracy: 0.1008 - val_loss: 2.8431e-04 - val_accuracy: 0.0914\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=5,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "    \n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=5,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 재사용 \n",
    "- 모델B.clone_model(모델A) \n",
    "- 모델B.set_weights(모델A.get_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 읽어옴 \n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "\n",
    "# model_B_on_A와 model_A는 층을 공유\n",
    "# 작업 B를 위한 모델 : model_B_on_A \n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "\n",
    "# 출력층 레이어 추가 \n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_B_on_A와 model_A는 층을 공유\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "\n",
    "# model_A의 가중치를 가져옴 \n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_B_on_A 새로운 레이어 : 입력 레이어 ~ 출력 직전 레이어 학습 동결 \n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 출력 직전 레이어 ~ 출력 레이어( 새로운 레이어 ) 가중치 학습 \n",
    "# 에포크 약간만 주고 학습 \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재사용된 층의 동결 해제, 모델 다시 학습 - 재사용된 층을 현재 학습을 튜닝하기 위해 훈련 \n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 재사용된 층의 동결을 해제 후, 학습률을 낮추어 재사용된 가중치 유지 \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-4),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
