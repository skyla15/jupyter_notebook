{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10 TensorFlow2 + Keras CNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = 1000\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-입력데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X_train, axis=(0, 1, 2))\n",
    "X_std = np.std(X_train, axis=(0, 1, 2))\n",
    "\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 32, 32, 3) (35000, 1) (15000, 32, 32, 3) (15000, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_tf = train_tf.shuffle(SHUFFLE).batch(BATCH_SIZE)\n",
    "\n",
    "val_tf = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 모델 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def my_model():\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    \n",
    "    x = Conv2D(32, (3,3), padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3,3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = my_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = Adam(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = Mean(name='val_loss')\n",
    "val_accuracy = SparseCategoricalAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Back Propagation 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(images, training=True)\n",
    "        \n",
    "        loss = loss_object(labels, outputs)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, outputs)\n",
    "    \n",
    "@tf.function\n",
    "def val_step(images, labels):\n",
    "    outputs = model(images, training=False)\n",
    "    v_loss = loss_object(labels, outputs)\n",
    "    \n",
    "    val_loss(v_loss)\n",
    "    val_accuracy(labels, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-학습하기 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv2d_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 001, Train_loss: 1.883, Train_acc: 31.520%, Val_loss: 1.574, Val_acc: 43.913%\n",
      "Epoch: 002, Train_loss: 1.522, Train_acc: 45.149%, Val_loss: 1.388, Val_acc: 49.607%\n",
      "Epoch: 003, Train_loss: 1.381, Train_acc: 50.666%, Val_loss: 1.287, Val_acc: 53.487%\n",
      "Epoch: 004, Train_loss: 1.283, Train_acc: 54.271%, Val_loss: 1.192, Val_acc: 57.347%\n",
      "Epoch: 005, Train_loss: 1.212, Train_acc: 56.926%, Val_loss: 1.130, Val_acc: 59.793%\n",
      "Epoch: 006, Train_loss: 1.132, Train_acc: 59.851%, Val_loss: 1.063, Val_acc: 62.147%\n",
      "Epoch: 007, Train_loss: 1.064, Train_acc: 62.303%, Val_loss: 1.029, Val_acc: 63.207%\n",
      "Epoch: 008, Train_loss: 1.009, Train_acc: 64.657%, Val_loss: 0.964, Val_acc: 65.640%\n",
      "Epoch: 009, Train_loss: 0.952, Train_acc: 66.389%, Val_loss: 0.915, Val_acc: 67.733%\n",
      "Epoch: 010, Train_loss: 0.908, Train_acc: 67.997%, Val_loss: 0.852, Val_acc: 70.580%\n",
      "Epoch: 011, Train_loss: 0.871, Train_acc: 69.457%, Val_loss: 0.848, Val_acc: 70.180%\n",
      "Epoch: 012, Train_loss: 0.837, Train_acc: 70.606%, Val_loss: 0.813, Val_acc: 71.513%\n",
      "Epoch: 013, Train_loss: 0.801, Train_acc: 72.006%, Val_loss: 0.798, Val_acc: 72.367%\n",
      "Epoch: 014, Train_loss: 0.769, Train_acc: 73.294%, Val_loss: 0.773, Val_acc: 72.927%\n",
      "Epoch: 015, Train_loss: 0.737, Train_acc: 74.206%, Val_loss: 0.757, Val_acc: 73.567%\n",
      "Epoch: 016, Train_loss: 0.703, Train_acc: 75.414%, Val_loss: 0.746, Val_acc: 74.073%\n",
      "Epoch: 017, Train_loss: 0.681, Train_acc: 76.280%, Val_loss: 0.715, Val_acc: 75.373%\n",
      "Epoch: 018, Train_loss: 0.654, Train_acc: 77.029%, Val_loss: 0.711, Val_acc: 75.773%\n",
      "Epoch: 019, Train_loss: 0.634, Train_acc: 77.886%, Val_loss: 0.710, Val_acc: 75.867%\n",
      "Epoch: 020, Train_loss: 0.604, Train_acc: 78.920%, Val_loss: 0.687, Val_acc: 76.667%\n",
      "Epoch: 021, Train_loss: 0.578, Train_acc: 79.940%, Val_loss: 0.670, Val_acc: 76.820%\n",
      "Epoch: 022, Train_loss: 0.558, Train_acc: 80.466%, Val_loss: 0.671, Val_acc: 77.267%\n",
      "Epoch: 023, Train_loss: 0.541, Train_acc: 81.000%, Val_loss: 0.663, Val_acc: 77.507%\n",
      "Epoch: 024, Train_loss: 0.517, Train_acc: 82.017%, Val_loss: 0.655, Val_acc: 77.880%\n",
      "Epoch: 025, Train_loss: 0.497, Train_acc: 82.523%, Val_loss: 0.666, Val_acc: 77.553%\n",
      "Epoch: 026, Train_loss: 0.479, Train_acc: 83.214%, Val_loss: 0.657, Val_acc: 77.800%\n",
      "Epoch: 027, Train_loss: 0.462, Train_acc: 83.643%, Val_loss: 0.645, Val_acc: 78.120%\n",
      "Epoch: 028, Train_loss: 0.437, Train_acc: 84.437%, Val_loss: 0.642, Val_acc: 78.573%\n",
      "Epoch: 029, Train_loss: 0.426, Train_acc: 84.943%, Val_loss: 0.650, Val_acc: 78.267%\n",
      "Epoch: 030, Train_loss: 0.406, Train_acc: 85.777%, Val_loss: 0.657, Val_acc: 78.507%\n",
      "Epoch: 031, Train_loss: 0.388, Train_acc: 86.549%, Val_loss: 0.636, Val_acc: 79.067%\n",
      "Epoch: 032, Train_loss: 0.371, Train_acc: 86.971%, Val_loss: 0.638, Val_acc: 79.207%\n",
      "Epoch: 033, Train_loss: 0.360, Train_acc: 87.351%, Val_loss: 0.644, Val_acc: 79.160%\n",
      "Epoch: 034, Train_loss: 0.347, Train_acc: 87.720%, Val_loss: 0.637, Val_acc: 79.540%\n",
      "Epoch: 035, Train_loss: 0.337, Train_acc: 88.026%, Val_loss: 0.653, Val_acc: 78.947%\n",
      "Epoch: 036, Train_loss: 0.317, Train_acc: 88.769%, Val_loss: 0.660, Val_acc: 78.847%\n",
      "Epoch: 037, Train_loss: 0.311, Train_acc: 89.074%, Val_loss: 0.654, Val_acc: 79.620%\n",
      "Epoch: 038, Train_loss: 0.295, Train_acc: 89.537%, Val_loss: 0.672, Val_acc: 78.873%\n",
      "Epoch: 039, Train_loss: 0.288, Train_acc: 89.817%, Val_loss: 0.646, Val_acc: 79.940%\n",
      "Epoch: 040, Train_loss: 0.269, Train_acc: 90.597%, Val_loss: 0.656, Val_acc: 79.907%\n",
      "Epoch: 041, Train_loss: 0.261, Train_acc: 90.980%, Val_loss: 0.649, Val_acc: 80.213%\n",
      "Epoch: 042, Train_loss: 0.250, Train_acc: 91.194%, Val_loss: 0.659, Val_acc: 79.773%\n",
      "Epoch: 043, Train_loss: 0.245, Train_acc: 91.329%, Val_loss: 0.658, Val_acc: 80.240%\n",
      "Epoch: 044, Train_loss: 0.239, Train_acc: 91.509%, Val_loss: 0.665, Val_acc: 80.153%\n",
      "Epoch: 045, Train_loss: 0.225, Train_acc: 91.923%, Val_loss: 0.657, Val_acc: 80.247%\n",
      "Epoch: 046, Train_loss: 0.219, Train_acc: 92.257%, Val_loss: 0.671, Val_acc: 80.120%\n",
      "Epoch: 047, Train_loss: 0.217, Train_acc: 92.374%, Val_loss: 0.677, Val_acc: 79.973%\n",
      "Epoch: 048, Train_loss: 0.204, Train_acc: 92.791%, Val_loss: 0.667, Val_acc: 80.520%\n",
      "Epoch: 049, Train_loss: 0.198, Train_acc: 93.111%, Val_loss: 0.698, Val_acc: 80.107%\n",
      "Epoch: 050, Train_loss: 0.187, Train_acc: 93.589%, Val_loss: 0.700, Val_acc: 80.153%\n",
      "Epoch: 051, Train_loss: 0.187, Train_acc: 93.497%, Val_loss: 0.687, Val_acc: 80.260%\n",
      "Epoch: 052, Train_loss: 0.184, Train_acc: 93.606%, Val_loss: 0.689, Val_acc: 80.547%\n",
      "Epoch: 053, Train_loss: 0.177, Train_acc: 93.800%, Val_loss: 0.704, Val_acc: 80.267%\n",
      "Epoch: 054, Train_loss: 0.171, Train_acc: 94.060%, Val_loss: 0.708, Val_acc: 80.620%\n",
      "Epoch: 055, Train_loss: 0.165, Train_acc: 94.077%, Val_loss: 0.710, Val_acc: 80.387%\n",
      "Epoch: 056, Train_loss: 0.165, Train_acc: 94.229%, Val_loss: 0.720, Val_acc: 79.987%\n",
      "Epoch: 057, Train_loss: 0.159, Train_acc: 94.374%, Val_loss: 0.706, Val_acc: 80.400%\n",
      "Epoch: 058, Train_loss: 0.151, Train_acc: 94.583%, Val_loss: 0.707, Val_acc: 80.800%\n",
      "Epoch: 059, Train_loss: 0.148, Train_acc: 94.726%, Val_loss: 0.705, Val_acc: 80.953%\n",
      "Epoch: 060, Train_loss: 0.148, Train_acc: 94.806%, Val_loss: 0.724, Val_acc: 80.500%\n",
      "Epoch: 061, Train_loss: 0.141, Train_acc: 95.069%, Val_loss: 0.720, Val_acc: 80.507%\n",
      "Epoch: 062, Train_loss: 0.142, Train_acc: 95.046%, Val_loss: 0.729, Val_acc: 80.473%\n",
      "Epoch: 063, Train_loss: 0.138, Train_acc: 95.120%, Val_loss: 0.725, Val_acc: 80.707%\n",
      "Epoch: 064, Train_loss: 0.134, Train_acc: 95.400%, Val_loss: 0.761, Val_acc: 80.293%\n",
      "Epoch: 065, Train_loss: 0.131, Train_acc: 95.420%, Val_loss: 0.758, Val_acc: 80.220%\n",
      "Epoch: 066, Train_loss: 0.132, Train_acc: 95.360%, Val_loss: 0.728, Val_acc: 80.447%\n",
      "Epoch: 067, Train_loss: 0.128, Train_acc: 95.560%, Val_loss: 0.739, Val_acc: 80.833%\n",
      "Epoch: 068, Train_loss: 0.122, Train_acc: 95.691%, Val_loss: 0.736, Val_acc: 80.580%\n",
      "Epoch: 069, Train_loss: 0.118, Train_acc: 95.869%, Val_loss: 0.755, Val_acc: 80.473%\n",
      "Epoch: 070, Train_loss: 0.119, Train_acc: 95.783%, Val_loss: 0.741, Val_acc: 80.813%\n",
      "Epoch: 071, Train_loss: 0.119, Train_acc: 95.829%, Val_loss: 0.742, Val_acc: 80.827%\n",
      "Epoch: 072, Train_loss: 0.116, Train_acc: 96.080%, Val_loss: 0.759, Val_acc: 80.267%\n",
      "Epoch: 073, Train_loss: 0.111, Train_acc: 96.103%, Val_loss: 0.761, Val_acc: 80.787%\n",
      "Epoch: 074, Train_loss: 0.111, Train_acc: 96.254%, Val_loss: 0.780, Val_acc: 80.727%\n",
      "Epoch: 075, Train_loss: 0.104, Train_acc: 96.309%, Val_loss: 0.766, Val_acc: 80.793%\n",
      "Epoch: 076, Train_loss: 0.108, Train_acc: 96.143%, Val_loss: 0.767, Val_acc: 80.853%\n",
      "Epoch: 077, Train_loss: 0.103, Train_acc: 96.366%, Val_loss: 0.780, Val_acc: 80.827%\n",
      "Epoch: 078, Train_loss: 0.105, Train_acc: 96.311%, Val_loss: 0.762, Val_acc: 80.813%\n",
      "Epoch: 079, Train_loss: 0.105, Train_acc: 96.489%, Val_loss: 0.770, Val_acc: 80.980%\n",
      "Epoch: 080, Train_loss: 0.099, Train_acc: 96.571%, Val_loss: 0.772, Val_acc: 81.013%\n",
      "Epoch: 081, Train_loss: 0.100, Train_acc: 96.557%, Val_loss: 0.768, Val_acc: 81.320%\n",
      "Epoch: 082, Train_loss: 0.095, Train_acc: 96.731%, Val_loss: 0.779, Val_acc: 80.993%\n",
      "Epoch: 083, Train_loss: 0.096, Train_acc: 96.723%, Val_loss: 0.764, Val_acc: 81.220%\n",
      "Epoch: 084, Train_loss: 0.092, Train_acc: 96.797%, Val_loss: 0.780, Val_acc: 81.173%\n",
      "Epoch: 085, Train_loss: 0.089, Train_acc: 96.954%, Val_loss: 0.801, Val_acc: 81.000%\n",
      "Epoch: 086, Train_loss: 0.093, Train_acc: 96.754%, Val_loss: 0.783, Val_acc: 81.073%\n",
      "Epoch: 087, Train_loss: 0.089, Train_acc: 97.003%, Val_loss: 0.775, Val_acc: 81.173%\n",
      "Epoch: 088, Train_loss: 0.089, Train_acc: 96.894%, Val_loss: 0.776, Val_acc: 81.353%\n",
      "Epoch: 089, Train_loss: 0.085, Train_acc: 97.040%, Val_loss: 0.770, Val_acc: 81.207%\n",
      "Epoch: 090, Train_loss: 0.088, Train_acc: 96.920%, Val_loss: 0.770, Val_acc: 81.293%\n",
      "Epoch: 091, Train_loss: 0.085, Train_acc: 97.077%, Val_loss: 0.800, Val_acc: 80.993%\n",
      "Epoch: 092, Train_loss: 0.088, Train_acc: 97.006%, Val_loss: 0.777, Val_acc: 81.133%\n",
      "Epoch: 093, Train_loss: 0.080, Train_acc: 97.200%, Val_loss: 0.813, Val_acc: 81.253%\n",
      "Epoch: 094, Train_loss: 0.084, Train_acc: 97.174%, Val_loss: 0.819, Val_acc: 80.633%\n",
      "Epoch: 095, Train_loss: 0.079, Train_acc: 97.197%, Val_loss: 0.768, Val_acc: 81.600%\n",
      "Epoch: 096, Train_loss: 0.082, Train_acc: 97.240%, Val_loss: 0.792, Val_acc: 81.433%\n",
      "Epoch: 097, Train_loss: 0.080, Train_acc: 97.314%, Val_loss: 0.798, Val_acc: 80.987%\n",
      "Epoch: 098, Train_loss: 0.078, Train_acc: 97.323%, Val_loss: 0.795, Val_acc: 81.347%\n",
      "Epoch: 099, Train_loss: 0.079, Train_acc: 97.357%, Val_loss: 0.796, Val_acc: 81.233%\n",
      "Epoch: 100, Train_loss: 0.074, Train_acc: 97.540%, Val_loss: 0.805, Val_acc: 81.287%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    \n",
    "    for images, labels in train_tf:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    for val_images, val_labels in val_tf:\n",
    "        val_step(val_images, val_labels)\n",
    "        \n",
    "    print('Epoch: {:03d}, Train_loss: {:.3f}, Train_acc: {:.3%}, Val_loss: {:.3f}, Val_acc: {:.3%}'.format(\n",
    "        epoch + 1,\n",
    "        train_loss.result(),\n",
    "        train_accuracy.result(),\n",
    "        val_loss.result(),\n",
    "        val_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
