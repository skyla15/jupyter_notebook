{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB 긍부정 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaeyeopchung/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/jaeyeopchung/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "decode_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 입력 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # 크기가 sequences, dimension 이고 원소가 0인 행렬을 만들어 준다. \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1, # result[i]에서 특정 인덱스 위치를 1로 만들어 준다. \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels).astype('float32')\n",
    "y_test = np.array(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.compile(tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Val_loss' )\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Val_acc' )\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적은층 많은층 모델 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_big.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_big = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = models.Sequential([\n",
    "    layers.Dense(4, activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "    \n",
    "model_small.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_small = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "acc_big = history_big.history['accuracy']\n",
    "val_acc_big = history_big.history['val_accuracy']\n",
    "loss_big = history_big.history['loss']\n",
    "val_loss_big = history_big.history['val_loss']\n",
    "\n",
    "acc_small = history_small.history['accuracy']\n",
    "val_acc_small = history_small.history['val_accuracy']\n",
    "loss_small = history_small.history['loss']\n",
    "val_loss_small = history_small.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Train_acc')\n",
    "plt.plot(epochs, acc_big, 'r', label='Train_acc_big')\n",
    "plt.plot(epochs, acc_small, 'g', label='Train_acc_small')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'b', label='Train_loss')\n",
    "plt.plot(epochs, loss_big, 'r', label='Train_loss_big')\n",
    "plt.plot(epochs, loss_small, 'g', label='Train_loss_small')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1, L2 Norm 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_l2 = models.Sequential([\n",
    "    layers.Dense(16, kernel_regularizer=regularizers.l2(0.01), activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(16, kernel_regularizer=regularizers.l2(0.01), activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_l2.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_l2 = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2_val_loss = history_l2.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, val_loss, 'b', label='Val_loss')\n",
    "plt.plot(epochs, model_l2_val_loss, 'r', label='L2_Val_loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_l1 = models.Sequential([\n",
    "    layers.Dense(16, kernel_regularizer=regularizers.l1(0.01), activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(16, kernel_regularizer=regularizers.l1(0.01), activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_l1.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_l1 = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_val_loss = history_l2.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, val_loss, 'b', label='Val_loss')\n",
    "plt.plot(epochs, model_l1_val_loss, 'r', label='L1_Val_loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop = models.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(10000,)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_drop.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_drop = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop_val_loss = history_drop.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, val_loss, 'b', label='Val_loss')\n",
    "plt.plot(epochs, model_drop_val_loss, 'r', label='Drop_Val_loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타이타닉 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_m_1 = titanic[(titanic['Sex']=='male') & (titanic['Pclass']==1)]['Age'].mean()\n",
    "med_m_2 = titanic[(titanic['Sex']=='male') & (titanic['Pclass']==2)]['Age'].mean()\n",
    "med_m_3 = titanic[(titanic['Sex']=='male') & (titanic['Pclass']==3)]['Age'].mean()\n",
    "med_f_1 = titanic[(titanic['Sex']=='female') & (titanic['Pclass']==1)]['Age'].mean()\n",
    "med_f_2 = titanic[(titanic['Sex']=='female') & (titanic['Pclass']==2)]['Age'].mean()\n",
    "med_f_3 = titanic[(titanic['Sex']=='female') & (titanic['Pclass']==3)]['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.loc[(titanic['Age'].isnull()) & (titanic['Sex']=='male') & (titanic['Pclass']==1), 'Age'] = med_m_1\n",
    "titanic.loc[(titanic['Age'].isnull()) & (titanic['Sex']=='male') & (titanic['Pclass']==2), 'Age'] = med_m_2\n",
    "titanic.loc[(titanic['Age'].isnull()) & (titanic['Sex']=='male') & (titanic['Pclass']==3), 'Age'] = med_m_3\n",
    "titanic.loc[(titanic['Age'].isnull()) & (titanic['Sex']=='female') & (titanic['Pclass']==1), 'Age'] = med_f_1\n",
    "titanic.loc[(titanic['Age'].isnull()) & (titanic['Sex']=='female') & (titanic['Pclass']==2), 'Age'] = med_f_2\n",
    "titanic.loc[(titanic['Age'].isnull()) & (titanic['Sex']=='female') & (titanic['Pclass']==3), 'Age'] = med_f_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Embarked'] = titanic['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Cabin\"] = titanic[\"Cabin\"].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    59\n",
       "B    47\n",
       "D    33\n",
       "E    32\n",
       "A    15\n",
       "F    13\n",
       "G     4\n",
       "T     1\n",
       "Name: Cabin, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Cabin\"] = titanic[\"Cabin\"].fillna(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          891 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        891 non-null    object \n",
      " 11  Embarked     891 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del titanic[\"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del titanic[\"Ticket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Sex          891 non-null    object \n",
      " 4   Age          891 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Fare         891 non-null    float64\n",
      " 8   Cabin        891 non-null    object \n",
      " 9   Embarked     891 non-null    object \n",
      "dtypes: float64(2), int64(5), object(3)\n",
      "memory usage: 69.7+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
      " 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1\n",
      " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1\n",
      " 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
      " 0 1 1]\n",
      "['female' 'male']\n"
     ]
    }
   ],
   "source": [
    "Sex_E = LabelEncoder()   \n",
    "Sex_E.fit(titanic[\"Sex\"])\n",
    "Sex_EC = Sex_E.transform(titanic[\"Sex\"])\n",
    "print(Sex_EC)\n",
    "print(Sex_E.inverse_transform([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Sex\"] = Sex_EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 2 2 1 2 2 2 0 2 2 2 2 2 2 1 2 2 0 2 2 1 2 2 2 0 2 1 2 0 0 1 2 0 2 0\n",
      " 2 2 0 2 2 0 0 1 2 1 1 0 2 2 2 0 2 0 2 2 0 2 2 0 2 2 2 0 0 2 2 2 2 2 2 2 0\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 2 2 2 1 2\n",
      " 0 2 2 0 2 1 2 0 2 2 2 0 2 2 0 1 2 0 2 0 2 2 2 2 0 2 2 2 0 0 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 0 2 2 0 2 2 2 0 2 2 2\n",
      " 2 1 2 1 2 2 2 2 2 0 0 1 2 1 2 2 2 2 0 2 2 2 0 1 0 2 2 2 2 1 0 2 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 2 2 0 1 2 2 2 2 2 2 2 2 2 0 0 2 0\n",
      " 2 1 2 2 2 1 2 2 2 2 2 2 2 2 0 1 2 2 2 1 2 1 2 2 2 2 0 2 2 2 1 2 0 0 2 2 0\n",
      " 0 2 2 0 1 1 2 1 2 2 0 0 0 0 0 0 2 2 2 2 2 2 2 0 2 2 1 2 2 0 2 2 2 0 1 2 2\n",
      " 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 1 1 2 0 0 2 1 2 0 0 1 0\n",
      " 0 2 2 0 2 0 2 0 0 2 0 0 2 2 2 2 2 2 1 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 1 1 2 2 2 2 2 2 2 0 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 2 2 2 0 0 2 0 2 2 2 1 2 2 2 2 2 2 2 2 1 0 2 2 2 0 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 0 2 2 2 2 2 0 2 0 0 2 2 2 2 1 1 2 2 0 2 2 2 2 1 2 2 0 2 2 2 1\n",
      " 2 2 2 2 0 0 0 1 2 2 2 2 2 0 0 0 2 2 2 0 2 0 2 2 2 2 0 2 2 0 2 2 0 2 1 0 2\n",
      " 2 0 0 2 2 1 2 2 2 2 2 2 2 0 2 2 2 2 1 2 2 2 2 0 2 2 0 2 0 0 2 2 0 2 2 2 0\n",
      " 2 1 2 2 2 2 0 0 2 2 2 2 0 2 2 2 0 2 2 2 1 1 2 2 2 2 2 2 0 2 0 2 2 2 1 2 2\n",
      " 1 2 2 0 2 2 2 2 2 2 2 2 0 2 2 0 0 2 0 2 2 2 2 2 1 1 2 2 1 2 0 2 0 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 0 2 2 2 0 2 2 2 2 2 0 2 0 2 2 2 1 0 2 0 2 0\n",
      " 1 2 2 2 2 2 0 0 2 2 2 2 2 0 2 1 2 2 2 2 2 2 2 2 1 2 2 2 0 2 2 2 2 2 0 2 2\n",
      " 2 2 0 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 1 1 2 2 2 2 0 2 2 1\n",
      " 2 1 2 0 2 2 2 2 2 2 1 2 0 1 2 2 0 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 2 1 2 0 1 2 0 2 0 2 2 0 2 2 2 0 2 2 0 0 2 2 2 0 2 0 2\n",
      " 2 0 2 2 2 2 2 0 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0 0 2 2 2 0 2 2 2 2 2 1 2 2\n",
      " 2 0 1]\n",
      "['C' 'Q' 'S']\n"
     ]
    }
   ],
   "source": [
    "Embarked_E = LabelEncoder()   \n",
    "Embarked_E.fit(titanic[\"Embarked\"])\n",
    "Embarked_EC = Embarked_E.transform(titanic[\"Embarked\"])\n",
    "print(Embarked_EC)\n",
    "print(Embarked_E.inverse_transform([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Embarked\"] = Embarked_EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 4 2 2 2 6 2 2 2 2 2 2 2 2 2 2 3 2 0 2 2 2 2 2 2 2 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 1 2 2 2 2 2 2 1 2 2 2 2 5 2 2 2 2 2 2 2\n",
      " 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 0 3 2 2 2 2 3 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 1 2 2 2 2 4 3 2 2 2 5 2 2 2 2 2 2 2 3 2 2 1 2 2 2 2 2 2 2 2\n",
      " 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 1 2 2 2 0 2 2 2 2 2 2 2 2 5 2\n",
      " 0 2 2 2 2 2 2 2 5 1 1 2 2 2 2 2 2 2 2 2 6 2 2 2 0 2 2 2 2 2 3 2 2 3 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 6 2 2 2 2 2 1 2\n",
      " 2 2 2 4 1 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 1 3 2 2 2\n",
      " 2 2 2 1 2 2 2 4 2 2 2 2 2 4 2 1 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 3 2 1 2 2 2\n",
      " 2 2 2 2 4 2 7 5 2 2 2 2 5 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 3 2 2 1\n",
      " 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 3 6 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 4 1 2 2 2 2 2 2 2 2\n",
      " 2 0 2 2 2 2 2 2 2 2 2 2 4 3 2 2 4 2 4 2 2 2 2 2 2 2 2 2 2 3 2 0 2 2 2 2 2\n",
      " 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 4 2 2 3 5 2\n",
      " 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 0 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 4 2 2 2 2 2 0 2 4 2 1 2 2 2 3\n",
      " 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 2 2 3 2 2 2 3 2 3 2\n",
      " 2 0 2 1 2 2 2 2 2 2 2 2 1 2 2 2 3 2 0 2 2 2 2 2 2 2 2 2 2 2 3 2 2 4 2 2 2\n",
      " 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 3 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 5 2 4 2\n",
      " 2 2 2 2 4 2 2 2 2 2 2 2 5 2 4 2 2 2 2 2 2 4 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2\n",
      " 3 2 1 2 2 1 2 2 3 2 2 4 2 2 2 2 2 2 2 1 2 2 2 1 2 3 2 2 2 2 2 2 4 2 2 2 5\n",
      " 2 2 1 2 1 3 2 2 2 2 2 2 1 2 2 2 2 2 2 3 2 2 2 2 2 1 2 2 2 0 2 2 4 2 2 2 2\n",
      " 2 1 2 2 2 2 1 2 2 4 2 2 2 2 2 1 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 2 2 2 4 2 2 2 2 3 2 2 2 2 0 2 2 2 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1\n",
      " 2 2 2]\n",
      "['A' 'B' 'C' 'D' 'E']\n"
     ]
    }
   ],
   "source": [
    "Cabin_E = LabelEncoder()   \n",
    "Cabin_E.fit(titanic[\"Cabin\"])\n",
    "Cabin_EC = Cabin_E.transform(titanic[\"Cabin\"])\n",
    "print(Cabin_EC)\n",
    "print(Cabin_E.inverse_transform([0,1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Cabin\"] = Cabin_EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Sex          891 non-null    int32  \n",
      " 4   Age          891 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Fare         891 non-null    float64\n",
      " 8   Cabin        891 non-null    int32  \n",
      " 9   Embarked     891 non-null    int32  \n",
      "dtypes: float64(2), int32(3), int64(5)\n",
      "memory usage: 59.3 KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 9)\n",
      "(179, 9)\n",
      "(712,)\n",
      "(179,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train = titanic['Survived']\n",
    "x_train = titanic.drop('Survived', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=12) \n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.transpose(x_train)\n",
    "x_test = np.transpose(x_test)\n",
    "y_train = np.transpose(y_train)\n",
    "y_test = np.transpose(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 712)\n",
      "(9, 179)\n",
      "(712,)\n",
      "(179,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_test\n",
    "partial_x_train = x_train\n",
    "\n",
    "y_val = y_test\n",
    "partial_y_train = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/100\n",
      "712/712 [==============================] - 0s 496us/sample - loss: 4.6862 - accuracy: 0.5843 - val_loss: 7.6214 - val_accuracy: 0.4358\n",
      "Epoch 2/100\n",
      "712/712 [==============================] - 0s 95us/sample - loss: 5.8501 - accuracy: 0.5421 - val_loss: 5.9094 - val_accuracy: 0.5642\n",
      "Epoch 3/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 5.7492 - accuracy: 0.5295 - val_loss: 5.7766 - val_accuracy: 0.5698\n",
      "Epoch 4/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 4.8971 - accuracy: 0.5983 - val_loss: 4.6976 - val_accuracy: 0.5587\n",
      "Epoch 5/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 3.6434 - accuracy: 0.5969 - val_loss: 4.9291 - val_accuracy: 0.5642\n",
      "Epoch 6/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 3.6402 - accuracy: 0.6025 - val_loss: 3.6010 - val_accuracy: 0.6089\n",
      "Epoch 7/100\n",
      "712/712 [==============================] - 0s 84us/sample - loss: 3.1295 - accuracy: 0.6053 - val_loss: 4.5798 - val_accuracy: 0.5866\n",
      "Epoch 8/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 3.0001 - accuracy: 0.6320 - val_loss: 2.5677 - val_accuracy: 0.4469\n",
      "Epoch 9/100\n",
      "712/712 [==============================] - 0s 85us/sample - loss: 2.5677 - accuracy: 0.6236 - val_loss: 2.0781 - val_accuracy: 0.6089\n",
      "Epoch 10/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 2.9226 - accuracy: 0.6166 - val_loss: 3.2695 - val_accuracy: 0.5922\n",
      "Epoch 11/100\n",
      "712/712 [==============================] - 0s 85us/sample - loss: 2.1852 - accuracy: 0.6573 - val_loss: 3.5693 - val_accuracy: 0.5922\n",
      "Epoch 12/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 2.5051 - accuracy: 0.6194 - val_loss: 2.7477 - val_accuracy: 0.6145\n",
      "Epoch 13/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 2.4323 - accuracy: 0.6362 - val_loss: 4.7066 - val_accuracy: 0.5642\n",
      "Epoch 14/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 2.9282 - accuracy: 0.6053 - val_loss: 1.4516 - val_accuracy: 0.6257\n",
      "Epoch 15/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 2.3838 - accuracy: 0.5927 - val_loss: 1.6561 - val_accuracy: 0.5140\n",
      "Epoch 16/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 2.0010 - accuracy: 0.6475 - val_loss: 3.6427 - val_accuracy: 0.4246\n",
      "Epoch 17/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 2.0014 - accuracy: 0.6166 - val_loss: 4.1690 - val_accuracy: 0.5754\n",
      "Epoch 18/100\n",
      "712/712 [==============================] - 0s 84us/sample - loss: 2.2027 - accuracy: 0.6264 - val_loss: 1.0419 - val_accuracy: 0.6313\n",
      "Epoch 19/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 1.6676 - accuracy: 0.6362 - val_loss: 1.2657 - val_accuracy: 0.6480\n",
      "Epoch 20/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 1.6397 - accuracy: 0.6615 - val_loss: 3.6573 - val_accuracy: 0.5587\n",
      "Epoch 21/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 1.9689 - accuracy: 0.6376 - val_loss: 1.0263 - val_accuracy: 0.6313\n",
      "Epoch 22/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 1.4279 - accuracy: 0.6362 - val_loss: 3.8860 - val_accuracy: 0.4581\n",
      "Epoch 23/100\n",
      "712/712 [==============================] - ETA: 0s - loss: 3.6932 - accuracy: 0.48 - 0s 77us/sample - loss: 1.7577 - accuracy: 0.5997 - val_loss: 3.3376 - val_accuracy: 0.5754\n",
      "Epoch 24/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 1.3808 - accuracy: 0.6657 - val_loss: 2.3590 - val_accuracy: 0.5587\n",
      "Epoch 25/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 1.4684 - accuracy: 0.6208 - val_loss: 0.9592 - val_accuracy: 0.6480\n",
      "Epoch 26/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 1.1054 - accuracy: 0.6461 - val_loss: 2.8895 - val_accuracy: 0.5698\n",
      "Epoch 27/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 1.0990 - accuracy: 0.6896 - val_loss: 1.9253 - val_accuracy: 0.5698\n",
      "Epoch 28/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 1.3726 - accuracy: 0.6278 - val_loss: 2.3676 - val_accuracy: 0.4749\n",
      "Epoch 29/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 1.4078 - accuracy: 0.6503 - val_loss: 0.8414 - val_accuracy: 0.6313\n",
      "Epoch 30/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 1.0056 - accuracy: 0.6587 - val_loss: 1.3866 - val_accuracy: 0.4916\n",
      "Epoch 31/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 1.2679 - accuracy: 0.6376 - val_loss: 0.8945 - val_accuracy: 0.6201\n",
      "Epoch 32/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 1.0354 - accuracy: 0.6728 - val_loss: 2.2389 - val_accuracy: 0.4860\n",
      "Epoch 33/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.9967 - accuracy: 0.6657 - val_loss: 1.0946 - val_accuracy: 0.6313\n",
      "Epoch 34/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 1.4051 - accuracy: 0.6166 - val_loss: 0.8289 - val_accuracy: 0.6592\n",
      "Epoch 35/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.9104 - accuracy: 0.6826 - val_loss: 2.8312 - val_accuracy: 0.5084\n",
      "Epoch 36/100\n",
      "712/712 [==============================] - 0s 85us/sample - loss: 1.2137 - accuracy: 0.6320 - val_loss: 0.7672 - val_accuracy: 0.6480\n",
      "Epoch 37/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.9794 - accuracy: 0.6854 - val_loss: 1.1835 - val_accuracy: 0.6145\n",
      "Epoch 38/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.8691 - accuracy: 0.6840 - val_loss: 0.8172 - val_accuracy: 0.6592\n",
      "Epoch 39/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 0.9001 - accuracy: 0.6728 - val_loss: 1.4308 - val_accuracy: 0.6369\n",
      "Epoch 40/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.8525 - accuracy: 0.6952 - val_loss: 1.1556 - val_accuracy: 0.6257\n",
      "Epoch 41/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.9534 - accuracy: 0.6517 - val_loss: 0.7318 - val_accuracy: 0.6201\n",
      "Epoch 42/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.7419 - accuracy: 0.7051 - val_loss: 3.0111 - val_accuracy: 0.5810\n",
      "Epoch 43/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.8889 - accuracy: 0.6671 - val_loss: 1.0055 - val_accuracy: 0.5196\n",
      "Epoch 44/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.7085 - accuracy: 0.6728 - val_loss: 0.8025 - val_accuracy: 0.6201\n",
      "Epoch 45/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 0.7886 - accuracy: 0.7093 - val_loss: 0.8714 - val_accuracy: 0.6480\n",
      "Epoch 46/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.8983 - accuracy: 0.6728 - val_loss: 1.5199 - val_accuracy: 0.5140\n",
      "Epoch 47/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.8206 - accuracy: 0.6868 - val_loss: 0.7083 - val_accuracy: 0.6704\n",
      "Epoch 48/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.7615 - accuracy: 0.6713 - val_loss: 1.9481 - val_accuracy: 0.5754\n",
      "Epoch 49/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.8985 - accuracy: 0.6671 - val_loss: 1.6452 - val_accuracy: 0.5922\n",
      "Epoch 50/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.6357 - accuracy: 0.7317 - val_loss: 1.0171 - val_accuracy: 0.5196\n",
      "Epoch 51/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 0.9340 - accuracy: 0.6615 - val_loss: 1.0708 - val_accuracy: 0.5978\n",
      "Epoch 52/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.6796 - accuracy: 0.7135 - val_loss: 1.6613 - val_accuracy: 0.5140\n",
      "Epoch 53/100\n",
      "712/712 [==============================] - 0s 71us/sample - loss: 0.7054 - accuracy: 0.7022 - val_loss: 1.1026 - val_accuracy: 0.6089\n",
      "Epoch 54/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.7337 - accuracy: 0.6966 - val_loss: 0.7272 - val_accuracy: 0.6927\n",
      "Epoch 55/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.6786 - accuracy: 0.6980 - val_loss: 1.2164 - val_accuracy: 0.5978\n",
      "Epoch 56/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.7363 - accuracy: 0.6924 - val_loss: 1.2159 - val_accuracy: 0.5028\n",
      "Epoch 57/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.6112 - accuracy: 0.7331 - val_loss: 0.6916 - val_accuracy: 0.6313\n",
      "Epoch 58/100\n",
      "712/712 [==============================] - 0s 72us/sample - loss: 0.6826 - accuracy: 0.7065 - val_loss: 0.8192 - val_accuracy: 0.6425\n",
      "Epoch 59/100\n",
      "712/712 [==============================] - 0s 86us/sample - loss: 0.7044 - accuracy: 0.7191 - val_loss: 1.2727 - val_accuracy: 0.6034\n",
      "Epoch 60/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.6710 - accuracy: 0.7247 - val_loss: 1.2807 - val_accuracy: 0.5866\n",
      "Epoch 61/100\n",
      "712/712 [==============================] - 0s 84us/sample - loss: 0.5970 - accuracy: 0.7317 - val_loss: 0.7738 - val_accuracy: 0.6816\n",
      "Epoch 62/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.7168 - accuracy: 0.7107 - val_loss: 0.8182 - val_accuracy: 0.6257\n",
      "Epoch 63/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.6883 - accuracy: 0.7177 - val_loss: 1.8436 - val_accuracy: 0.5866\n",
      "Epoch 64/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.6088 - accuracy: 0.7444 - val_loss: 0.8066 - val_accuracy: 0.6760\n",
      "Epoch 65/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.7220 - accuracy: 0.7219 - val_loss: 0.7987 - val_accuracy: 0.6369\n",
      "Epoch 66/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.6228 - accuracy: 0.7416 - val_loss: 0.7204 - val_accuracy: 0.6760\n",
      "Epoch 67/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.6240 - accuracy: 0.7135 - val_loss: 0.8741 - val_accuracy: 0.6480\n",
      "Epoch 68/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.6173 - accuracy: 0.7303 - val_loss: 1.0548 - val_accuracy: 0.5140\n",
      "Epoch 69/100\n",
      "712/712 [==============================] - 0s 72us/sample - loss: 0.6379 - accuracy: 0.7051 - val_loss: 0.9518 - val_accuracy: 0.6257\n",
      "Epoch 70/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5878 - accuracy: 0.7514 - val_loss: 0.9995 - val_accuracy: 0.5922\n",
      "Epoch 71/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.6084 - accuracy: 0.7219 - val_loss: 0.7528 - val_accuracy: 0.6648\n",
      "Epoch 72/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 0.5806 - accuracy: 0.7556 - val_loss: 0.8714 - val_accuracy: 0.6201\n",
      "Epoch 73/100\n",
      "712/712 [==============================] - 0s 85us/sample - loss: 0.5659 - accuracy: 0.7472 - val_loss: 0.7308 - val_accuracy: 0.6983\n",
      "Epoch 74/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.6067 - accuracy: 0.7275 - val_loss: 0.8189 - val_accuracy: 0.6480\n",
      "Epoch 75/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 0.5894 - accuracy: 0.7233 - val_loss: 1.3456 - val_accuracy: 0.5810\n",
      "Epoch 76/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5521 - accuracy: 0.7458 - val_loss: 0.9489 - val_accuracy: 0.4972\n",
      "Epoch 77/100\n",
      "712/712 [==============================] - 0s 82us/sample - loss: 0.6047 - accuracy: 0.7219 - val_loss: 1.1870 - val_accuracy: 0.5531\n",
      "Epoch 78/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.5932 - accuracy: 0.7177 - val_loss: 0.7208 - val_accuracy: 0.6816\n",
      "Epoch 79/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5992 - accuracy: 0.7360 - val_loss: 0.7475 - val_accuracy: 0.6872\n",
      "Epoch 80/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5104 - accuracy: 0.7669 - val_loss: 0.8167 - val_accuracy: 0.6257\n",
      "Epoch 81/100\n",
      "712/712 [==============================] - 0s 71us/sample - loss: 0.6043 - accuracy: 0.7149 - val_loss: 0.6892 - val_accuracy: 0.7095\n",
      "Epoch 82/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.6037 - accuracy: 0.7346 - val_loss: 0.7577 - val_accuracy: 0.6983\n",
      "Epoch 83/100\n",
      "712/712 [==============================] - 0s 83us/sample - loss: 0.5487 - accuracy: 0.7626 - val_loss: 1.1140 - val_accuracy: 0.6034\n",
      "Epoch 84/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4637 - accuracy: 0.7893 - val_loss: 0.7266 - val_accuracy: 0.6257\n",
      "Epoch 85/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.5758 - accuracy: 0.7275 - val_loss: 1.0415 - val_accuracy: 0.6089\n",
      "Epoch 86/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4998 - accuracy: 0.7781 - val_loss: 0.6593 - val_accuracy: 0.6872\n",
      "Epoch 87/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 0.4976 - accuracy: 0.7626 - val_loss: 0.9842 - val_accuracy: 0.5978\n",
      "Epoch 88/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.5027 - accuracy: 0.7753 - val_loss: 1.0354 - val_accuracy: 0.6257\n",
      "Epoch 89/100\n",
      "712/712 [==============================] - 0s 81us/sample - loss: 0.5492 - accuracy: 0.7374 - val_loss: 0.7578 - val_accuracy: 0.6816\n",
      "Epoch 90/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4886 - accuracy: 0.7570 - val_loss: 0.6889 - val_accuracy: 0.6536\n",
      "Epoch 91/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.5174 - accuracy: 0.7514 - val_loss: 0.7223 - val_accuracy: 0.6816\n",
      "Epoch 92/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.5522 - accuracy: 0.7275 - val_loss: 0.7284 - val_accuracy: 0.6425\n",
      "Epoch 93/100\n",
      "712/712 [==============================] - 0s 93us/sample - loss: 0.5102 - accuracy: 0.7753 - val_loss: 0.8526 - val_accuracy: 0.6089\n",
      "Epoch 94/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.5055 - accuracy: 0.7865 - val_loss: 0.6946 - val_accuracy: 0.6480\n",
      "Epoch 95/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.5310 - accuracy: 0.7683 - val_loss: 0.7057 - val_accuracy: 0.6816\n",
      "Epoch 96/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.5184 - accuracy: 0.7528 - val_loss: 0.7403 - val_accuracy: 0.6480\n",
      "Epoch 97/100\n",
      "712/712 [==============================] - 0s 85us/sample - loss: 0.5133 - accuracy: 0.7640 - val_loss: 1.2526 - val_accuracy: 0.5754\n",
      "Epoch 98/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.5029 - accuracy: 0.7711 - val_loss: 0.7096 - val_accuracy: 0.7207\n",
      "Epoch 99/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.5350 - accuracy: 0.7725 - val_loss: 1.2933 - val_accuracy: 0.6089\n",
      "Epoch 100/100\n",
      "712/712 [==============================] - 0s 94us/sample - loss: 0.5365 - accuracy: 0.7654 - val_loss: 0.8632 - val_accuracy: 0.6592\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(9,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.compile(tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=100,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
