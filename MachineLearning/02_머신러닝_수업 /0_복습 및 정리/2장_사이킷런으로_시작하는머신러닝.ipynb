{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0. 사이킷런의 기반 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 사이킷런 클래스\n",
    "    - fit()\n",
    "    - predict() \n",
    "1. Estimator 클래스\n",
    "    1.1. 분류 : classifier\n",
    "    1.2. 회귀 알고리즘 : Regressor \n",
    "2. Evaluation 함수 : Estimator를 일자로 받고, fit, predict를 통해 평가, 하이퍼파라미터 튜닝 등 \n",
    "    2.1. cross_val_score() \n",
    "    2.2. GridSearchCV 등 \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. sklearn.model_selection 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 학습/데이터 세트 분리 : train_test_split() \n",
    "from sklearn.model_selection import train_test_split\n",
    "- 인자 \n",
    "    - 첫 번째 파라미터 : feature data set \n",
    "    - 두 번째 파라미터 : 종속 데이터 세트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - test_size : 테스트 세트 크기 비율 설정\n",
    "    - shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을 지 결정 \n",
    "    - random_state = n : \n",
    "        - 랜덤 시드와 같은 개념 \n",
    "        - 설정하지 않을 경우, train_test_split() 호출시마다 무작위 분리\n",
    "        - 일정한 숫자를 지정하여 동일한 데이터 세트가 나오도록 설정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### 학습 / 데이터 세트 분리하고 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier( )\n",
    "iris_data = load_iris()\n",
    "\n",
    "X_iris = iris_data['data']\n",
    "y_iris = iris_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_iris[:10]) # 종속 데이터 \n",
    "print(y_iris[:10]) # 타겟 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.9467\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터, 테스트 데이터 분리\n",
    "# 테스트 데이터 비율 = 0.5, Seed = 121 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, \n",
    "                                                    test_size=0.5, random_state=121)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "######  학습 / 데이터 세트 분리하지 않고 학습 \n",
    "- 예측 결과가 100% 정확한 이유는 이미 학습한학습 데이터 세트를 기반으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "train_data = iris.data\n",
    "train_label = iris.target\n",
    "dt_clf.fit(train_data, train_label)\n",
    "\n",
    "# 학습 데이터 셋으로 예측 수행\n",
    "pred = dt_clf.predict(train_data)\n",
    "print('예측 정확도:',accuracy_score(train_label,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1.2. 교차 검증 ( KFold, StratifiedKFold )\n",
    "\n",
    "from sklearn.model_selection import KFold / StratifiedKFold\n",
    "- 과적합(Overfitting) : 모델이 학습데이터에 과도하게 최적화되어, 실제 예측 데이터를 사용 시, 학습 데이터의 노이즈, 아웃라이어 등에 의해 성능이 매우 떨어지는 것 \n",
    "- 교차 검증 \n",
    "    - \"여러 개\"의 학습 데이터와, 테스터 데이터 세트로 나누어 학습, 평가 수행 \n",
    "    - 각 세트의 수행 평가 결과에 따라, 하이퍼 파라미터 튜닝 등의 모델 최적화 더 쉽게 가능 \n",
    "<img src = './img/교차검증.jpg' width = '70%' height = '70%' align = 'left' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2.1 K-Fold Cross Validation\n",
    "- K = 5인 경우 \n",
    "    - 첫 번째 반복에서는처음부터 4개 등분을학습데이터 세트, 마지막5번째 등분하나를검증데이터 세트로설정하고학습데이터 세트에 서 학습수행 검증데이터 세트에서 평가를 수행\n",
    "    - 5개(K개)의 예측 평가를 구했으면 이를 평균해서 K 폴드 평가 결과\n",
    "<img src = './img/kfold.jpg' width = '70%' height = '70%' align = 'left' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "붓꽃 데이터 세트 크기: 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_accuracy = []\n",
    "print('붓꽃 데이터 세트 크기:',features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도 :1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "#1 검증 세트 인덱스:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "#2 교차 검증 정확도 :0.9667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "#2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59]\n",
      "\n",
      "#3 교차 검증 정확도 :0.8667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "#3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89]\n",
      "\n",
      "#4 교차 검증 정확도 :0.9333, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "#4 검증 세트 인덱스:[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "\n",
      "#5 교차 검증 정확도 :0.7333, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
      "#5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 평균 검증 정확도: 0.9\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "\n",
    "# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  \n",
    "for train_index, test_index  in kfold.split(features):\n",
    "    \n",
    "    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "        # 각 fold별, 학습용 세트의 feature, 검증 세트의 feature \n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "        # 각 fold별, 학습용 세트의 종속 변수, 검증 세트의 종속 변수\n",
    "    \n",
    "    \n",
    "    #학습 및 예측 \n",
    "    dt_clf.fit(X_train , y_train)\n",
    "        # 학습용 feature와 종속변수를 이용 학습 \n",
    "    pred = dt_clf.predict(X_test)\n",
    "        # decision tree classifier를 이용하여, 예측 수행 \n",
    "    \n",
    "    n_iter += 1\n",
    "    \n",
    "    # 반복 시 마다 정확도 측정 \n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    # 테스트 셋의 실제값과, 예측값 비교 \n",
    "    # accuracy_score(accuracy_score(검증 데이터의 실제값, 예측된 값))\n",
    "    \n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'\n",
    "          .format(n_iter, accuracy, train_size, test_size))\n",
    "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "    \n",
    "# 개별 iteration별 정확도를 합하여 평균 정확도 계산 \n",
    "print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2.2 Stratified K  폴드\n",
    "- 분포가 고르지 못한 데이터를 학습/검증 데이터로 K개 분할 할 경우, 모든 데이터 세트가 동일한 분포를 갖도록 데이터를 분할 \n",
    "- 한쪽으로 편향된 데이터를 갖고 있는 데이터를 학습시킬 경우,\n",
    "    - K-Fold를 이용할 경우, 랜덤하게 학습, 검증 세트를 선택하더라도, 특정 feature들의 특성이 제대로 반영이 안될 수 있음.\n",
    "- 일반적인 분류모델에서는 Stratified K Fold 사용하여야 함 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 편향된(Skewed, Imbalanced) 데이터 Staratified K-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state = 10)\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "n_iter=0\n",
    "k_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "features = iris['data']\n",
    "label = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c2d0ff5f6547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlabel_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0miris_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlabel_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0miris_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iris_df' is not defined"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(features, label):\n",
    "    n_iter += 1\n",
    "    label_train= iris_df['label'].iloc[train_index]\n",
    "    label_test= iris_df['label'].iloc[test_index]\n",
    "    \n",
    "    \n",
    "    print('========== 교차 검증: {0} =========='.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n', label_test.value_counts())\n",
    "    \n",
    "    \n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    k_mean.append(accuracy)\n",
    "    \n",
    "    print('{0} ➤ 교차 검증 정확도 : {1}'.format(n_iter, accuracy))\n",
    "    \n",
    "    k = int('31' + str(n_iter))\n",
    "    plt.subplot(k)\n",
    "    sns.distplot(features[train_index])\n",
    "    \n",
    "print('Strtified K-Fold 평균 정확도 :', np.mean(k_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 편향된(Skewed, Imbalanced) 데이터에서의 K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "iris = load_iris()\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['label']=iris.target\n",
    "iris_df['label'].value_counts()\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=3)\n",
    "# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환. \n",
    "n_iter =0\n",
    "for train_index, test_index  in kfold.split(iris_df):\n",
    "    n_iter += 1\n",
    "    label_train= iris_df['label'].iloc[train_index]\n",
    "    label_test= iris_df['label'].iloc[test_index]\n",
    "    print('==========  교차 검증: {0} =========='.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n', label_test.value_counts())\n",
    "    k = int('31' + str(n_iter))\n",
    "    plt.subplot(k)\n",
    "    sns.distplot(features[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "sns.distplot(iris_df)\n",
    "plt.subplot(212)\n",
    "sns.distplot(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 교차 검증을 보다 간편하게 ( cross_val_score ) \n",
    "from sklearn.model_selection import cross_val_scroe, cross_validate\n",
    "- Estimator를 학습(fit), 예측(predict), 평가(evaluation)시켜주므로 간단하게 교차 검증을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cross_val_score(estimator, X, y, scoring, cv)\n",
    "    - estimator \n",
    "        - 분류: Classifier\n",
    "        - 회귀: Regressor \n",
    "    - X : 피처 데이터 세트\n",
    "    - y : 검증 데이터 세트 \n",
    "    - scoring : 예측 성능 평가 지표 \n",
    "    - cv : Fold 수 \n",
    "2. 반환값 : scroing 파라미터에 따른 성능 지표 측정값\n",
    "3. 분류 방법 \n",
    "    - Regressor : K-Fold 방식 \n",
    "    - Classifier : Stratified K-Fold 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도: [0.98 0.94 0.98]\n",
      "평균 검증 정확도: 0.9667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score , cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris_data = load_iris()\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "data = iris_data.data\n",
    "label = iris_data.target\n",
    "\n",
    "# 성능 지표는 정확도(accuracy) , 교차 검증 세트(cv)는 3개 \n",
    "scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)\n",
    "print('교차 검증별 정확도:',np.round(scores, 4))\n",
    "print('평균 검증 정확도:', np.round(np.mean(scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 검증과 최적 하이퍼 파라미터 튜닝 GridSearchCV \n",
    "- 데이터 세트를 cross-validation을 위한 학습/테스트 세트로 자동 분할하여, 여러 조합의 하이퍼 파라미터를 적용하여 최적의 파라미터를 찾는다. \n",
    "- CV = 3 ( 3개의 세트로 나누어 교차 검증 )이라면, 개별 하이퍼 파라미터 조합마다 각 폴딩 세트를 학습/평가하여성능을 평가 \n",
    "    - ex) CV = 3, 하이퍼 파라미터 조합 = 6 이라면, 각 폴딩 세트마다 6번, 총 18회의 학습/ 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    \n",
    "- GridSearchCV(estimator, param_grid, scoring, cv, refit) \n",
    "    - estimator : Classifier, regressor, pipeline 등 \n",
    "    - param_grid : 키:리스트 형태 딕셔너리, estimatior의 튜닝을 위해 사용될 파라미터 \n",
    "    - cv : 교차 검증을 위해 사용되는 학습/테스트 세트 개수 \n",
    "    - refit : True -> 최적의 하이퍼파라미터 찾으면, estimator객체를 재학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리\n",
    "iris = load_iris()\n",
    "X_iris = iris['data']\n",
    "y_iris = iris['target']\n",
    "\n",
    "# 학습, 테스트 데이터 분리 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, \n",
    "                                                    test_size=0.2, random_state=121)\n",
    "\n",
    "# Deicistion Tree Classirfier ( Estimator 클래스 )\n",
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 3}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     params  mean_test_score  rank_test_score  \\\n",
       "0  {'max_depth': 1, 'min_samples_split': 2}         0.700000                5   \n",
       "1  {'max_depth': 1, 'min_samples_split': 3}         0.700000                5   \n",
       "2  {'max_depth': 2, 'min_samples_split': 2}         0.958333                3   \n",
       "3  {'max_depth': 2, 'min_samples_split': 3}         0.958333                3   \n",
       "4  {'max_depth': 3, 'min_samples_split': 2}         0.975000                1   \n",
       "5  {'max_depth': 3, 'min_samples_split': 3}         0.975000                1   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  \n",
       "0              0.700                0.7               0.70  \n",
       "1              0.700                0.7               0.70  \n",
       "2              0.925                1.0               0.95  \n",
       "3              0.925                1.0               0.95  \n",
       "4              0.975                1.0               0.95  \n",
       "5              0.975                1.0               0.95  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### parameter 들을 dictionary{ 키 : 리스트 } 형태로 설정\n",
    "parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}\n",
    "\n",
    "# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  \n",
    "### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재학습 시킴.  \n",
    "grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)\n",
    "\n",
    "# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "# GridSearchCV 결과 추출하여 DataFrame으로 변환\n",
    "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "scores_df[['params', 'mean_test_score', 'rank_test_score', \\\n",
    "           'split0_test_score', 'split1_test_score', 'split2_test_score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rank_test_score : 하이퍼 파라미터 성능 순위\n",
    "- split0~2 _test_score : 각각 3개의 폴딩 세트에서 하이퍼 파라미터를 테스트한 성능 수치 \n",
    "- mean_test_score : split0~2_test_score 의 평균값 \n",
    "- 0~5 인덱스 : 하이퍼파라미터 조합을 보여줌 \n",
    "\n",
    "- GridSearchCV 객체의 fit( )을 수행 시, \n",
    "    - 최고 성능을 보이는 파라미터 값과 평가는 다음 변수에 저장이 됨 \n",
    "        - best_params_\n",
    "        - best_score_ \n",
    "    - 최고 성능을 나타내는 파라미터값으로 재학습된 estimator 객체는 다음과 같이 가져올 수 있음 \n",
    "        - best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('grid_dtree.best_score_ : ', grid_dtree.best_score_)\n",
    "print('grid_dtree.best_params_ : ', grid_dtree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid_dtree.best_estimator_ \n",
    "pred = best_estimator.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.1. 전처리 가이드 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1.1 전반적인 흐름\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "###### 1. 데이터 Type, Label 등이 일관적이지 않은 경우  \n",
    "    - 프로그램에서 제공하는 함수를 통해 일괄적으로 변경 (예, SQL: Cast, Python: astype())\n",
    "    - 날짜 : pandas datetime으로 변환\n",
    "\n",
    "###### 2. 데이터 Type, Label 등이 일관적이지 않은 경우  \n",
    "프로그램에서 제공하는 함수를 통해 일괄적으로 변경 (예, SQL: Cast, Python: astype())\n",
    "\n",
    "###### 3. Missing Value\n",
    "    - 샘플이 많은 경우 : 삭제해도 무관\n",
    "    - 샘플이 적은 경우 \n",
    "        - 카테고리형인 경우 Mode로 채우거나 분류 예측 모델링 활용 (예, Logistic Regression)\n",
    "        - 수치형인 경우 Mean, Median 등 대푯값으로 채우거나 실수 예측 모델링 활용 (예, Linear Regression)\n",
    "    - 회귀 및 분류 예측 모델 이용 가능 \n",
    "\n",
    "###### 4. 텍스트 처리\n",
    "    - Errors, Typo 발생의 경우 : 텍스트 처리 함수 활용 (예, Python: str.replace())\n",
    "    - 같은 의미를 갖는 카테고리들 또한 동일하게 맞춰줄 필요 있음 \n",
    "\n",
    "###### 5. 이상치(outlier)  \n",
    "IQR, Z-score, MAD 등 방식으로 이상치 제거\n",
    "\n",
    "###### 6. 변수가 많은 경우(20개 이상)  \n",
    "PCA 등으로 차원 축소하거나 변수 중요도 파악후 불필요 변수 제거\n",
    "\n",
    "###### 7. 편향된 분포의 변수가 존재하는 경우  \n",
    "log, sqrt 등 함수로 분포 변환\n",
    "측정 단위(scale)이 차이가 클 경우\n",
    "\n",
    "###### 8. StarndardScale or MinMaxScaler 통해 스케일링\n",
    "\n",
    "###### 9. 하이퍼 파라미터 튜닝\n",
    "\n",
    "###### 10. 다중공선성 데이터 확인 \n",
    "    - 독립변수끼리 강한 상관관계를 가지면 다중공선성Multicollinearity이 있다고 함 \n",
    "    - 독립변수끼리 종속적인 것 자체가 회귀분석에 위배되고, 수치적인 문제 야기 \n",
    "    - 다중공선성이 있을 가능성이 높은 경우\n",
    "        1. F검정은 통과했으나 각각의 회귀계수가 t검정을 통과하지 못하는 경우\n",
    "        2. 예상하던 것과 달리 회귀계수의 부호가 반대일 정도로 괴리가 큰 경우\n",
    "        3. 데이터를 추가하거나 제거할 때 기존의 회귀계수가 극심하게 많이 변하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.1.2. 데이터 결측치 처리\n",
    "\n",
    "결측값 즉 비어있는 값이 있는 상태로 모델을 만들게 될 경우 변수간의 관계가 왜곡될 수 있기 때문에 모델의 정확성이 떨어지게 됩니다. 결측치 처리는 결측치 제거, 수치형의 경우 평균이나 중앙치로 대체하거나 범주형인 경우 mode 값으로 대체, 간단한 예측 모델로 대체하는 방식이 일반적으로 이용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 데이터 결측치 처리방법1 - 삭제\n",
    "결측값이 발생한 모든 관측치를 삭제하거나 데이터 중 모델에 포함시킬 변수 들 중 관측값이 발생한 모든 관측치를 삭제하는 방법이 있습니다. 그러나 전체삭제 또는 부분삭제는 실제 예측에 영향을 주는 데이터일 경우 Cost에 영향을 미칠 수 있습니다.\n",
    "\n",
    "그렇기 때문에 삭제는 결측값이 무작위로 발생한 경우에 사용합니다. 결측값이 무작위로 발생한 것이 아닌데 삭제할 경우 왜곡된 모델이 생성될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 데이터 결측치 처리방법2 - 다른 값으로 대체\n",
    "결측값이 발생한 경우 다른 관측치의 평균, 최빈값, 중간값으로 대체할 수 있습니다. 결측 값이 발생이 다른 변수와 관계가 있는 경우 유용하짐나 그렇지 않은 경우 모델이 왜곡될 가능성이 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 데이터 결측치 처리방법3 - 예측값 삽입\n",
    "결측값이 없는 관측치를 트레이닝 데이터로 사용해서 예측모델을 만드는 방법입니다. 예측하는 방법은 Regression이나 Logistic Regression을 주로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1.3. 이상 데이터 처리\n",
    "\n",
    "이상 데이터란 기존 데이터와 동떨어진 관측치로, 모델을 왜곡할 가능성이 많은 데이터들입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 이상 데이터 처리방법1 - 단순삭제\n",
    "이상데이터가 실수로 발생한 경우에는 해당 값을 삭제하면 됩니다. 예를 들어 단순 오타나 비현실적인 응답 등입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 이상 데이터 처리방법2 - 다른 값으로 대체\n",
    "    1) 표준점수로 변환 후 -3 이하 및 +3 제거 \n",
    "    2) IQR 방식 \n",
    "    3) 도메인 지식 이용하거나 Binning 처리하는 방식 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.2. 데이터 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder   \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.3.1. 레이블 인코딩  => 카레고리 데이터 분석 모델 (Classifier) 적용, Regressor 적용 시 오류 많아짐 \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "- 같은 문자열을 갖는 값은 같은 숫자로 라벨링 됨 \n",
    "- 카데고리 피처를 코드형숫자 값으로 변환   \n",
    "- 몇몇 ML 알고리즘에는 이를 적용할 경우 예측 성능이 떨어지는 경우가 발생 가능 \n",
    "    - 같은 특징을 갖는 데이터임에도, 레이블링될 때 숫자의 크고 작고가 생기기에 가중치로 여겨질 수 있음\n",
    "    \n",
    "<img src = './img/라벨인코딩.jpg' width = '60%' height = '60%' align = 'left' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "items = [ 'TV', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print('           labels : ', labels)\n",
    "inversed = encoder.inverse_transform(labels)\n",
    "print('inverse_transform : ', inversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.3.2. 원-핫 인코딩 => 적용 전 라벨 인코딩 + 2차원 데이터로 변환 => regressor 모델 사용 \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "- 문자열은 인코딩 불가능 => 모두 숫자로 미리 변환시켜줘야됨 \n",
    "- 피처값의 유형에 따라 새로운 피처를 추가해 고유값에 해당하는 칼럼에만 1 을 표시 하고나머지칼럼에는0을표시\n",
    "<img src = './img/원핫인코딩.jpg' width = '60%' height = '60%' align = 'left' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import pandas as pd\n",
    "\n",
    "items = [ 'TV', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
    "\n",
    "# 라벨 인코딩 : 문자열을 숫자형으로 라벨링 \n",
    "l_encoder = LabelEncoder() \n",
    "l_encoder.fit(items)\n",
    "labels = l_encoder.transform(items)\n",
    "\n",
    "# 2차원 데이터로 변환 \n",
    "labels = labels.reshape(-1,1)\n",
    "\n",
    "# 원핫 인코딩 : 0,1로 변환 \n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "\n",
    "print( '원본 : \\n', items )\n",
    "print( '라벨 인코딩 : \\n', labels.reshape(-1).tolist() )\n",
    "print( '원핫 인코딩 :') \n",
    "print(pd.DataFrame(oh_labels.toarray()))\n",
    "print( '원핫 인코딩 inverse : \\n', oh_encoder.inverse_transform(oh_labels).reshape(-1).tolist() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### 원-핫 인코딩 == pd.getdummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame({'items' : [ 'TV', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서'] })\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 피쳐 스케일링과 정규화 / 표준화 (1)\n",
    "import sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "1. 피쳐 스케일링 : 서로 다른 변수의 값 범위를일정한 수준으로 맞추는 작업\n",
    "\n",
    "1.1. 표준화 : 피쳐 ~ N(0,1) 변환  \n",
    "    $$ xi\\_new = { x - mean(x) \\over stdev(x)} $$\n",
    "\n",
    "          \n",
    "1.2. 정규화 : 서로 다른 피처의 크기를통일하기 위해 크기를변환해주는 개념\n",
    "\n",
    "  \n",
    "ex) A는거리를나타내는 변수로서 값이 0 ~ l00KM /  B는금액을나타내는속성 으로 값이 0 ~ 100,000,000,000원  \n",
    "    이 변수들을 모두 동일한 크기 단위로 비교하기 위해 값을모두최소 0 ~ 1의 값으로 변환하는 것\n",
    "\n",
    "$$ xi\\_new = { xi - min(x) \\over max(x) - min(x) }$$  \n",
    "\n",
    "\n",
    "\n",
    "2. 사이킷런의 Normalizer ( 정규화 모듈 ) :   \n",
    "선형대수에서의 정규화 개념이 적용 ➤ 개별 벡터를 모든 피처 벡터의 크기로 나누 줌 \n",
    "$$ xi\\_new = { xi \\over \\sqrt{xi^2 + yi^2 + zi^2} } $$\n",
    "\n",
    "##### 3. 피쳐 스케일링시 유의점 \n",
    "- 학습 데이터와 검증 데이터를 따로 스케일링할 경우, 스케일링 되는 비율이 맞지 않는 문제가 발생할 수 있고, 이는 올바르지 않은 예측 결과 도출\n",
    "- 학습 데이터에 fit을 적용했다면, 테스트 데이터는 fit을 하지말고, 학습 데이터에 맞춰져 있는 scaler 객체를 사용해야 같은 비율로 스케일링이 됨 \n",
    "\n",
    "- ( 아래의 예시 ) : 학습데이터에 fit 적용 후, 테스트 데이터에 새로 fit 적용, MinMaxScaler 적용\n",
    "    - 학습 데이터 : 0 ~ 10 범위를 갖음 \n",
    "        - scaler.fit(train_data) => 0 ~ 1 사이의 값을 갖게 됨, 결과적으로 1/10 스케일링 적용  \n",
    "    - 테스트 데이터 : 0 ~ 5 범위를 갖음 \n",
    "        - scaler.fit(train_data) => 0 ~ 0.5 사이의 값을 갖게 됨, 결과적으로 1/5 스케일링 적용 \n",
    "- 테스트 데이터에 다시 fit( )을 적용해서는 안 되며 학습 데이 터로 이미 fit( )이 적용된 Scaler 객체를 이용해 transform( )으로 변환\n",
    "\n",
    "#### 4. 요약\n",
    "###### - 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이더로 분리\n",
    "###### - 테스트 데이터 변환 시에는 fit( )이나 fit_transform( )을 적용않고 학습 데이터로 이미 fit( )된 Scaler 객체를 이용해 transform( )으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# 학습 데이터는 0 부터 10까지, 테스트 데이터는 0 부터 5까지 값을 가지는 데이터 세트로 생성\n",
    "# Scaler클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경\n",
    "train_array = np.arange(0, 11).reshape(-1, 1)\n",
    "test_array =  np.arange(0, 6).reshape(-1, 1)\n",
    "\n",
    "# 최소값 0, 최대값 1로 변환하는 MinMaxScaler객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# fit()하게 되면 train_array 데이터의 최소값이 0, 최대값이 10으로 설정.  \n",
    "scaler.fit(train_array)\n",
    "# 1/10 scale로 train_array 데이터 변환함. 원본 10-> 1로 변환됨.\n",
    "train_scaled = scaler.transform(train_array)\n",
    " \n",
    "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
    "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))\n",
    "\n",
    "# 앞에서 생성한 MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최소값이 0, 최대값이 5으로 설정됨 \n",
    "scaler.fit(test_array)\n",
    "\n",
    "# 1/5 scale로 test_array 데이터 변환함. 원본 5->1로 변환.  \n",
    "test_scaled = scaler.transform(test_array)\n",
    "\n",
    "# train_array 변환 출력\n",
    "print('\\n원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))\n",
    "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))\n",
    "print('\\n학습 데이터와 테스트 데이터의 스케일링이 맞지 않음\\n학습 : 1 -> 0.1   //  테스트 : 5 -> 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# train_array에 Scale fit(), 테스트 세트도 이 Scaler를 사용해서 변환해야함 !!!!!!!!\n",
    "scaler.fit(train_array)\n",
    "train_scaled = scaler.transform(train_array)\n",
    "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
    "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))\n",
    "\n",
    "# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform() 만으로 변환해야 함. \n",
    "test_scaled = scaler.transform(test_array)\n",
    "\n",
    "print('\\n원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))\n",
    "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))\n",
    "print('\\n학습 데이터와 테스트 데이터의 스케일링이 맞음\\n학습 : 1 -> 0.1   //  테스트 : 5 -> 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3.1. StandardScaler : 표준화 지원, 개별 피쳐를 N(0,1)을 따르는 분포로 변환 / 반환형식 : ndarray\n",
    "import sklearn.preprocessing import StandardScaler\n",
    "- 사이킷런에서 데이터의 정규분포( N(0,1) )를 가정하고 구현된 모델\n",
    "    - 선형 회귀 ( Linear Regression )\n",
    "    - 로지스틱 회귀 ( Logistic Regression ) \n",
    "    - RBF 커널을 이용하는 SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "\n",
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  \n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature 들의 평균 값')\n",
    "print(iris_df_scaled.mean())\n",
    "print('\\nfeature 들의 분산 값')\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. MinMaxScaler : 데이터가 가우시안 분포를 따르지 않을 경우 적용, 0 ~ 1 사이 범위값으로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.  \n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('feature들의 최소 값')\n",
    "print(iris_df_scaled.min())\n",
    "\n",
    "print('\\nfeature들의 최대 값')\n",
    "print(iris_df_scaled.max())\n",
    "\n",
    "print('\\nfeature들의 평균 값')\n",
    "print(iris_df_scaled.mean())\n",
    "\n",
    "print('\\nfeature들의 분산 값')\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
