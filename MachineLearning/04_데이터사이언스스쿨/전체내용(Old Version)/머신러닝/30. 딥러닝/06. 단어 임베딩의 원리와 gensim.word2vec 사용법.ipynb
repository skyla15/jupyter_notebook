{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "af2546eaeb714d609bfd191e53b75fd0"
   },
   "source": [
    "# 단어 임베딩의 원리와 gensim.word2vec 사용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d7b18a950b864118948e4cc1511051f6"
   },
   "source": [
    "단어 임베딩(Word Embedding)이란 텍스트를 구성하는 하나의 단어를 수치화하는 방법의 일종이다.\n",
    "\n",
    "텍스트 분석에서 흔히 사용하는 방식은 단어 하나에 인덱스 정수를 할당하는 Bag of Words 방법이다. 이 방법을 사용하면 문서는 단어장에 있는 단어의 갯수와 같은 크기의 벡터가 되고 단어장의 각 단어가 그 문서에 나온 횟수만큼 벡터의 인덱스 위치의 숫자를 증가시킨다.\n",
    "\n",
    "즉 단어장이 \"I\", \"am\", \"a\", \"boy\", \"girl\" 다섯개의 단어로 이루어진 경우 각 단어에 다음과 같이 숫자를 할당한다.\n",
    "\n",
    "```\n",
    "\"I\": 0\n",
    "\"am\": 1\n",
    "\"a\": 2\n",
    "\"boy\": 3 \n",
    "\"girl\": 4\n",
    "```\n",
    "\n",
    "이 때 \"I am a girl\" 이라는 문서는 다음과 같이 벡터로 만들 수 있다.\n",
    "\n",
    "$$ [1 \\; 1 \\; 1 \\; 0 \\; 1] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "f2cf17f8158440709f180c02de520974"
   },
   "source": [
    "단어 임베딩은 하나의 단어를 하나의 인덱스 정수가 아니라 실수 벡터로 나타낸다. 예를 들어 2차원 임베딩을 하는 경우 다음과 같은 숫자 벡터가 될 수 있다.\n",
    "\n",
    "```\n",
    "\"I\": (0.3, 0.2)\n",
    "\"am\": (0.1, 0.8)\n",
    "\"a\": (0.5, 0.6)\n",
    "\"boy\": (0.2, 0.9) \n",
    "\"girl\": (0.4, 0.7)\n",
    "```\n",
    "\n",
    "단어 임베딩이 된 경우에는 각 단어 벡터를 합치거나(concatenation) 더하는(averaging, normalized Bag of Words) 방식으로 전체 문서의 벡터 표현을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e61573e85a2b49099701aeecbd864258"
   },
   "source": [
    "## Feed-Forward 신경망 언어 모형 (Neural Net Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2fc1886688fe4817ac27ffd33a127fb4"
   },
   "source": [
    "이러한 단어 임베딩은 신경망을 이용하여 언어 모형을 만들려는 시도에서 나왔다. 자세한 내용은 다음 논문을 참고한다.\n",
    "\n",
    "* \"A Neural Probabilistic Language Model\", Bengio, et al. 2003\n",
    "  * http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "* \"Efficient Estimation of Word Representations in Vector Space\", Mikolov, et al. 2013\n",
    "  * https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "  \n",
    "* \"word2vec Parameter Learning Explained\", Xin Rong, \n",
    "   * http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf\n",
    "\n",
    "V개의 단어를 가지는 단어장이 있을 때, 단어를 BOW 방식으로 크기 V인 벡터로 만든 다음 다음 그림과 같이 하나의 은닉층(Hidden Layer)을 가지는 신경망을 사용하여 특정 단어 열(word sequence)이 주어졌을 때 다음에 나올 단어를 예측하는 문제를 생각해 보자. 입력과 출력은 모두 BOW 방식으로 인코딩되어 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5fccbaf1155e4efba7a6ad9b974c616a"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/cd100ec8d3d6476e9522ead4c2acf6a2.png\" style=\"width: 50%;\">\n",
    "\n",
    "<small>이미지 출처: \"word2vec Parameter Learning Explained\", Xin Rong</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "95d6218d803241109b93647714081d4c"
   },
   "source": [
    "입력 $x$가 들어가면 입력 가중치 행렬 $W^T$이 곱해져서 은닉층 벡터 $h$가 되는데 $x$가 one-hot-encoding 된 값이므로 $h$ 벡터는 입력 가중치 행렬  $W$의 행 하나가 된다. \n",
    "\n",
    "$$ h = W^T x = v^T_i $$\n",
    "\n",
    "여기에서 $i$는 입력 벡터 $x$ 의 값이 1인 원소의 인덱스이다. 즉, BOW 단어장에서 $i$번째 단어를 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "1b97736380e74376b2ea4ff9c96044f4"
   },
   "source": [
    "벡터 $h$는 다시 출력 가중치 행렬 $W'^T$와 곱해져서 출력 벡터 $y$가 된다. \n",
    "\n",
    "$$ y = W'^T h $$\n",
    "\n",
    "출력 가중치 행렬 $W'$의 $j$번째 열을 $v_j$라고 하면 출력 벡터 $y$의 $j$번째 원소의 값은 다음과 같다.\n",
    "\n",
    "$$ y_j = v_j'^T h $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5d18b6d207d14acebe7834ba632d94da"
   },
   "source": [
    "가중치 행렬을 갱신하는 최적화 공식을 살펴본다. 자세한 유도과정은 논문을 참조한다.\n",
    "\n",
    "우선 출력 가중치 행렬의 갱신 공식은 다음과 같다.\n",
    "\n",
    "$$ v_j'^{\\text{(new)}} = v_j'^{\\text{(old)}} - \\eta \\cdot e_j \\cdot h = v_j'^{\\text{(old)}} - \\eta \\cdot e_j \\cdot v_i^T $$ \n",
    "\n",
    "이 식에서 $\\eta$는 최적화 스텝 사이즈, $e_j$는 출력 오차가 된다. 이 공식에 따르면 벡터 $v'_j$는 $v_j$ 방향으로 수렴해 간다. 즉, $i$번째 단어와 $j$번째 단어가 연속하는 관계라면 $v'_j$가 $v_i$와 유사한 위치로 수렴한다는 뜻이다.\n",
    "\n",
    "다음으로 입력 가중치 행렬의 갱신 공식은 다음과 같다.\n",
    "\n",
    "$$ v_i^{\\text{(new)}} = v_i^{\\text{(old)}} - \\eta \\sum_k e_j \\cdot w'_{ik}$$ \n",
    "\n",
    "이 공식에 따르면 벡터 $v_i$는 여러 $v'_k$ 벡터의 가중합으로 수렴해 간다. 이렇게 단어간의 관계에 의해 $i$번째 단어를 뜻하는 $v_i$의 값들이 연관성을 가지게 되는데 이 $v_i$ 벡터 값을 해당 단어에 대한 **분산 표현 (distributed representation)** , **벡터 표현 (vector representation)** 또는 **단어 임베딩 (word embedding)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b00d58b0b2b34bad9afa265ee67af719"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/versions/r0.11/images/linear-relationships.png\" style=\"width: 100%;\">\n",
    "\n",
    "<small>이미지 출처: https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "9fcfb718f20f4dec98d7a4fffb2061f0"
   },
   "source": [
    "## CBOW (Continuous Bag of Words) Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "639f85f1ddb541d7a10bc888c12c1745"
   },
   "source": [
    "위의 방식은 하나의 단어로부터 다음에 오는 단어를 예측하는 문제였다. 이러한 문제를 단어 하나짜리 문맥(single-word context)를 가진다고 한다. \n",
    "\n",
    "CBOW (Continuous Bag of Words) 방식은 복수 단어 문맥(multi-word context)에 대한 문제 즉, 여러개의 단어를 나열한 뒤 이와 관련된 단어를 추정하는 문제이다. 즉, 문자에서 나오는 $n$개의 단어 열로부터 다음 단어를 예측하는 문제가 된다. 예를 들어\n",
    "\n",
    "> the quick brown fox jumped over the lazy dog\n",
    "\n",
    "라는 문장에서 (`the`, `quick`, `brown`) 이라는 문맥이 주어지면 `fox`라는 단어를 예측해야 한다.\n",
    "\n",
    "CBOW는 다음과 같은 신경망 구조를 가진다. 여기에서 각 문맥 단어를 은닉층으로 투사하는 가중치 행렬은 모든 단어에 대해 공통으로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "76773d85902b496c9ca8dbdb2b2e5b4e"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/3cdbdbfe1c8a4742aaf2e0c40917948f.png\" style=\"width: 50%;\">\n",
    "\n",
    "<small>이미지 출처: \"word2vec Parameter Learning Explained\", Xin Rong</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "a3233d93b0a74b1eb8829f28cfac64b8"
   },
   "source": [
    "## Skip-Gram Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "3f8d3ef986f9418e86d5b40e6b27ffaf"
   },
   "source": [
    "Skip-Gram 방식은 CBOW 방식과 반대로 특정한 단어로부터 문맥이 될 수 있는 단어를 예측한다. 보통 입력 단어 주변의 $k$개 단어를 문맥으로 보고 예측 모형을 만드는데 이 $k$ 값을 window size 라고 한다.\n",
    "\n",
    "위 문장에서 window size $k=1$인 경우,\n",
    "\n",
    "* `quick` -> `the`\n",
    "* `quick` -> `brown`\n",
    "* `brown` -> `quick`\n",
    "* `brown` -> `fox`\n",
    "\n",
    "과 같은 관계를 예측할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "6d0676c0507c48699b6a9e204d2d19ca"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/8f8ebfa0ebb34eb584d24d59fe60a12d.png\" style=\"width: 50%;\">\n",
    "\n",
    "<small>이미지 출처: \"word2vec Parameter Learning Explained\", Xin Rong</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "efc5236e08e0402a891fbbd2be0dcdaf"
   },
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "8b3a965a5c4844a092d2743e250ed402"
   },
   "source": [
    "word2vec은 CBOW 방식과 Skip-Gram 방식의 단어 임베딩을 구현한 C++ 라이브러리로 구글에 있던 Mikolov 등이 개발하였다.\n",
    "\n",
    "파이썬에서는 gensim이라는 패키지에 `Word2Vec`이라는 클래스로 구현되어 있다. nltk의 영화 감상 corpus를 기반으로 `Word2Vec` 사용법을 살펴보자.\n",
    "\n",
    "우선 단어 임베딩을 위한 코퍼스를 만든다. 코퍼스는 리스트의 리스트 형태로 구현되어야 한다. 내부 리스트는 하나의 문장을 이루는 단어 열이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "aee9c11e539d4c42aacdc2b5f97fee24"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "080fb5c706ed4bdb960005490b58b581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'plot',\n",
       " u':',\n",
       " u'two',\n",
       " u'teen',\n",
       " u'couples',\n",
       " u'go',\n",
       " u'to',\n",
       " u'a',\n",
       " u'church',\n",
       " u'party',\n",
       " u',',\n",
       " u'drink',\n",
       " u'and',\n",
       " u'then',\n",
       " u'drive',\n",
       " u'.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5e42fec8d1d24de38a2be451905b0912"
   },
   "source": [
    "다음으로 이 코퍼스를 입력 인수로 하여 Word2Vec 클래스 객체를 생성한다. 이 시점에 트레이닝이 이루어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "5ff45ffa1fd64cb4b33f835a97db93b9"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "aa38bb935f634e6fa7cf6f3c1d1f24bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 400 ms, total: 12.6 s\n",
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "06100fecf7df4f3ea6f82604179ef7b9"
   },
   "source": [
    "트레이닝이 완료되면 `init_sims` 명령으로 필요없는 메모리를 unload 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "42d399a46c9e475f8db32b6733808587"
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e7bd1b3ac6c1447a9fe3da7ee28e7108"
   },
   "source": [
    "이제 이 모형에서 다음과 같은 메서드를 사용할 수 있다. 보다 자세한 내용은 https://radimrehurek.com/gensim/models/word2vec.html 를 참조한다.\n",
    "\n",
    "* `similarity` : 두 단어의 유사도 계산\n",
    "* `most_similar` : 가장 유사한 단어를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "b9e1191253f841b089c1466c823d08fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87472425755991945"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('actor', 'actress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "7454de6adad842a88fd14d99b3835b2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85470770334392587"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('he', 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "c0fb0c18d40846cc9f7d343e9ff0c737"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21756392610362227"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('actor', 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "0121ff0a224042228159cb5bc2c46bb8",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hero', 0.7978197932243347),\n",
       " (u'doctor', 0.7952470779418945),\n",
       " (u'actress', 0.7806568145751953),\n",
       " (u'performer', 0.775442361831665),\n",
       " (u'charming', 0.7602461576461792),\n",
       " (u'impression', 0.7583950757980347),\n",
       " (u'commoner', 0.7538788318634033),\n",
       " (u'janitor', 0.7536816000938416),\n",
       " (u'dude', 0.7528475522994995),\n",
       " (u'genius', 0.7506694793701172)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"villain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "eee28eee971644419e234be941223180"
   },
   "source": [
    "`most_similar` 메서드는 `positive` 인수와 `negative` 인수를 사용하여 다음과 같은 단어간 관계도 찾을 수 있다.\n",
    "\n",
    "> actor + he - actress = she"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "59e5ef54b5834576840005a621b3dae5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'she', 0.2471558153629303)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['actor', 'he'], negative='actress', topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "a8c6ae5c7e8c441c8575caf4aa51827d"
   },
   "source": [
    "이번에는 네이버 영화 감상 코퍼스를 사용하여 한국어 단어 임베딩을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "c8cc4e084e514788bec407f7e93e5345"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]   # header 제외\n",
    "    return data\n",
    "\n",
    "train_data = read_data('/home/dockeruser/data/nsmc/ratings_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "ba67db28bd3044d4b58f6a6da85a10ad"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "tagger = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_docs = [row[1] for row in train_data]\n",
    "sentences = [tokenize(d) for d in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "377c933373164127845eb435e7ba680b"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "f2dcf3a45e394db286a38002cbb805f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6062297706048696"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(*tokenize(u'악당 영웅'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "023a6ea6ae654987829afe9b462883e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0041346659756955097"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(*tokenize(u'악당 감동'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "school_cell_uuid": "06b4407cf7364fef9ec5f8b1986ea174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(여자/Noun, 0.6258430480957031)]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.utils import pprint\n",
    "pprint(model.most_similar(positive=tokenize(u'배우 남자'), negative=tokenize(u'여배우'), topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d707cf8a56174171a919326b5c3f05eb"
   },
   "source": [
    "더 많은 한국어 코퍼스를 사용한 단어 임베딩 모형은 다음 웹사이트에서 테스트해 볼 수 있다.\n",
    "* http://w.elnn.kr/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}